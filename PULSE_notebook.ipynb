{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the PULSE quantized keyword spotting notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure you have the following modules (the versions listed are the ones tested, may also work on others):\n",
    "- tensorflow 2.13\n",
    "- larq 0.13.3\n",
    "- numpy 1.23.5\n",
    "- librosa 0.10.0\n",
    "- matplotlib 3.7.1\n",
    "- scikit-learn 1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:33.930852Z",
     "start_time": "2023-05-02T15:46:33.925395Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import random\n",
    "import larq as lq\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from IPython.display import Audio\n",
    "from numpy import ndarray\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "tf.config.list_physical_devices()\n",
    "print(\"larq version: \", lq.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained on the Google Speech Commands v2 dataset, which can be downloaded from [here](https://www.tensorflow.org/datasets/catalog/speech_commands).\n",
    "\n",
    "Since we are using only parts of the dataset in our use case, we suggest copying/moving the needed folders of the whole dataset to a new folder, and specifying that folder as the DATASET_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:33.943929Z",
     "start_time": "2023-05-02T15:46:33.934924Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset Path\n",
    "\n",
    "DATASET_PATH = \"./current_classes\"\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != '.DS_Store']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code, the datased is loaded. The main parameter that could need changing is the BATCH_SIZE (default 32). The dataset is then split into training and validation sets, with the default split being 80% training and 20% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load audio files and split them into train, validation and test sets.\n",
    "BATCH_SIZE = 32\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "from tensorflow.python.data.ops.dataset_ops import BatchDataset\n",
    "train_ds: BatchDataset\n",
    "val_ds: BatchDataset\n",
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    output_sequence_length=SAMPLE_RATE,\n",
    "    seed=123,\n",
    "    subset=\"both\",\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start to preprocess the audio samples. First we remove the last empty dimension and plot a random sample as a waveform. Then we define 3 functions to be used in the preprocessing pipeline:\n",
    "\n",
    "-add_noises\n",
    "\n",
    "-time_shift\n",
    "\n",
    "-pitch_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:34.773265Z",
     "start_time": "2023-05-02T15:46:33.943739Z"
    }
   },
   "outputs": [],
   "source": [
    "#Squeeze the audio to remove the last dimension\n",
    "\n",
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "label_names = train_ds.class_names\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE, deterministic=True)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the first audio file\n",
    "\n",
    "def plot_waveform(audio):\n",
    "    audio = audio.numpy().flatten()\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(audio)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:34.932592Z",
     "start_time": "2023-05-02T15:46:34.777139Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function for adding noises\n",
    "\n",
    "#Load noise files\n",
    "pink_noise_file = './_background_noise_/pink_noise.wav'\n",
    "white_noise_file = './_background_noise_/white_noise.wav'\n",
    "\n",
    "pink_noise_audio, _ = librosa.load(pink_noise_file, sr=16000)\n",
    "white_noise_audio, _ = librosa.load(white_noise_file, sr=16000)\n",
    "\n",
    "# Convert to tensors\n",
    "white_noise_tensor = tf.convert_to_tensor(white_noise_audio, dtype=tf.float32)\n",
    "pink_noise_tensor = tf.convert_to_tensor(pink_noise_audio, dtype=tf.float32)\n",
    "\n",
    "# Add noise to audio\n",
    "def add_noises(audio_tensor, noise_types=['pink'], noise_probs=[1], noise_levels=[0.01]):\n",
    "    for noise_type, noise_prob, noise_level in zip(noise_types, noise_probs, noise_levels):\n",
    "        if random.random() < noise_prob:\n",
    "            if noise_type == 'white':\n",
    "                noise_tensor = white_noise_tensor\n",
    "            elif noise_type == 'pink':\n",
    "                noise_tensor = pink_noise_tensor\n",
    "\n",
    "            batch_size, audio_len = tf.shape(audio_tensor)[0], tf.shape(audio_tensor)[1]\n",
    "            noise_len = int(tf.shape(noise_tensor)[0])\n",
    "            start = tf.random.uniform((batch_size,), 0, noise_len - audio_len, dtype=tf.int32)\n",
    "            indices = tf.expand_dims(tf.range(batch_size), axis=1)\n",
    "            starts = tf.concat([indices, tf.expand_dims(start, axis=1)], axis=1)\n",
    "            noise = tf.map_fn(lambda x: tf.slice(noise_tensor, [x[1]], [audio_len]), starts, dtype=tf.float32)\n",
    "            audio_tensor = audio_tensor + noise_level * noise\n",
    "    return audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for time shifting\n",
    "\n",
    "def time_shift(audio_tensor, shift_range=0.1):\n",
    "    batch_size, audio_len = tf.shape(audio_tensor)[0], tf.shape(audio_tensor)[1]\n",
    "    shift_amount = tf.cast(tf.cast(audio_len, tf.float32) * shift_range, tf.int32)\n",
    "    shift = tf.random.uniform((batch_size,), minval=-shift_amount, maxval=shift_amount, dtype=tf.int32)\n",
    "\n",
    "    def shift_audio(x):\n",
    "        audio, delta = x\n",
    "        paddings = tf.cond(\n",
    "            delta < 0,\n",
    "            lambda: ((-delta, 0),),\n",
    "            lambda: ((0, delta),)\n",
    "        )\n",
    "        return tf.pad(tf.slice(audio, [tf.math.maximum(0, delta)], [audio_len - tf.math.abs(delta)]), paddings)\n",
    "\n",
    "    shifted_audio = tf.map_fn(shift_audio, (audio_tensor, shift), dtype=tf.float32)\n",
    "    return shifted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for pitch shifting\n",
    "\n",
    "def pitch_shift(audio_tensor, pitch_range=(0.3, 0.3), sample_rate=16000):\n",
    "    def shift_pitch(x):\n",
    "        audio = x.numpy()\n",
    "        factor = np.random.uniform(pitch_range[0], pitch_range[1])\n",
    "        n_bins = 12  # You can adjust this value based on your requirements\n",
    "        shifted_audio = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=factor, bins_per_octave=n_bins)\n",
    "        return tf.convert_to_tensor(shifted_audio, dtype=tf.float32)\n",
    "\n",
    "    return tf.py_function(shift_pitch, [audio_tensor], tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for applying all the preprocessing steps\n",
    "def preprocess_audio(audio, label, noise_colors=['pink'], noise_probs=[1], noise_levels=[0.01],\n",
    "                     shift_range=0.1,\n",
    "                     pitch_range=(-1, 1)):\n",
    "    audio = time_shift(audio, shift_range)\n",
    "    audio = add_noises(audio, noise_colors, noise_probs, noise_levels)\n",
    "    audio = pitch_shift(audio, pitch_range)\n",
    "    return audio, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block, we apply the preprocessing to train and validation datasets. Here you can change the following parameters:\n",
    "- **NOISE_COLORS**: a list with available noise types. Currently 'pink' and 'white' are supported.\n",
    "- **NOISE_PROBS**: a list with the probabilities of each noise type being applied. Should be between 0 and 1.\n",
    "- **NOISE_LEVELS**: a list with the noise intensity for each noise type. Default is 0.01.\n",
    "- **SHIFT_RANGE**: indicates how much the audio will be shifted in time. Default is 0.1.\n",
    "- **PITCH_RANGE**: indicates how much the audio will be shifted in pitch. Default is (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_COLORS = ['pink', 'white']\n",
    "NOISE_PROBS = [0.5, 0.5]\n",
    "NOISE_LEVELS = [0.01, 0.01]\n",
    "SHIFT_RANGE = 0.1\n",
    "PITCH_RANGE = (-1, 1)\n",
    "\n",
    "# Apply the preprocessing function to your train_ds and val_ds\n",
    "\n",
    "train_ds = train_ds.map(lambda audio, label: preprocess_audio(audio, label, NOISE_COLORS, NOISE_PROBS, NOISE_LEVELS, SHIFT_RANGE, PITCH_RANGE), tf.data.AUTOTUNE, deterministic=True)\n",
    "val_ds = val_ds.map(lambda audio, label: preprocess_audio(audio, label, NOISE_COLORS, NOISE_PROBS, NOISE_LEVELS, SHIFT_RANGE, PITCH_RANGE), tf.data.AUTOTUNE, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can hear a sample to check if the preprocessing is working as expected. Take note that in order to preview the data, the preprocessing effectively gets applied here for the first time and this block will take some time to execute. This is due to how Tensorflow internally maps transformations on the dataset (lazy mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('CPU: 0'):\n",
    "    random_batch = random.choice(list(train_ds))\n",
    "    random_audio_data, _ = random_batch\n",
    "    random_audio_data = random_audio_data.numpy()\n",
    "\n",
    "random_sample_index = random.randint(0, random_audio_data.shape[0] - 1)\n",
    "random_sample = random_audio_data[random_sample_index]\n",
    "audio_player = Audio(random_sample, rate=16000)\n",
    "\n",
    "audio_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half of validation dataset is moved to test dataset\n",
    "\n",
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can specify the parameters for computing the 2D representation of the audio data. The default method used is the Short Time Fourier Transform (STFT).\n",
    "\n",
    "Alternatively you can use the Mel Cepstral Coefficients (MFCC) method by setting MFCC=True, but this is not recommended as it is not as robust as STFT. \n",
    "\n",
    "You can modify the available parameters for each method, but the default ones should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectrogram parameters\n",
    "frame_length = 255 # 320\n",
    "frame_step = 128 #160\n",
    "fft_lenght = 256\n",
    "\n",
    "MFCC = False # Set to False to use spectrogram\n",
    "\n",
    "#MFCC parameters\n",
    "num_mel_bins = 40\n",
    "num_mfccs = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:36.349389Z",
     "start_time": "2023-05-02T15:46:36.219330Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define and compute the 2D representation of the audio\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=frame_length, frame_step=frame_step, fft_length=fft_lenght)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`)).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "def compute_mfccs(pcm, sample_rate, num_mel_bins, num_mfccs):\n",
    "    # A 1024-point STFT with frames of 64 ms and 75% overlap.\n",
    "    stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = tf.shape(stfts)[-1]\n",
    "    lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first num_mfccs.\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfccs]\n",
    "    \n",
    "    return mfccs\n",
    "\n",
    "\n",
    "\n",
    "def get_mfcc(waveform, sample_rate, num_mel_bins, num_mfccs):\n",
    "    # Compute MFCCs from the input waveform.\n",
    "    mfccs = compute_mfccs(waveform, sample_rate, num_mel_bins, num_mfccs)\n",
    "\n",
    "    # Add a `channels` dimension, so that the MFCCs can be used\n",
    "    # as input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`)).\n",
    "    mfccs = mfccs[..., tf.newaxis]\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "\n",
    "def make_spec_ds(ds: tf.data.Dataset):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE,\n",
    "      deterministic=True)\n",
    "\n",
    "def make_mfcc_ds(ds: tf.data.Dataset):\n",
    "    return ds.map(\n",
    "        map_func=lambda audio, label: (get_mfcc(audio, sample_rate = SAMPLE_RATE, num_mel_bins=num_mel_bins, num_mfccs=num_mfccs), label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=True)\n",
    "\n",
    "if(MFCC):\n",
    "    train_spectrogram_ds = make_mfcc_ds(train_ds)\n",
    "    val_spectrogram_ds = make_mfcc_ds(val_ds)\n",
    "    test_spectrogram_ds = make_mfcc_ds(test_ds)\n",
    "else:\n",
    "    train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "    val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "    test_spectrogram_ds = make_spec_ds(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code plots 9 samples after the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:39.986872Z",
     "start_time": "2023-05-02T15:46:38.967024Z"
    }
   },
   "outputs": [],
   "source": [
    "#Spectrogram plotting function\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  x = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  y = range(height)\n",
    "  ax.pcolormesh(x, y, log_spec)\n",
    "\n",
    "def plot_mfcc(mfcc, ax):\n",
    "    if len(mfcc.shape) > 2:\n",
    "        assert len(mfcc.shape) == 3\n",
    "        mfcc = np.squeeze(mfcc, axis=-1)\n",
    "    mfcc_data = np.swapaxes(mfcc, 0, 1)\n",
    "    ax.imshow(mfcc_data, interpolation='nearest', cmap=plt.cm.coolwarm, origin='lower', aspect='auto')\n",
    "    ax.set_title('MFCC')\n",
    "\n",
    "\n",
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break\n",
    "\n",
    "if(MFCC): print(\"Shape of MFCC: \", example_spectrograms.shape)\n",
    "else: print(\"Shape of Spectrogram: \", example_spectrograms.shape)\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    if(MFCC): plot_mfcc(example_spectrograms[i].numpy(), ax)\n",
    "    else: plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(label_names[example_spect_labels[i].numpy().argmax()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block of code you can toggle normalization of the images. This is disabled by default: with a quantized model we did not see any improvements by normalizing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:45.599166Z",
     "start_time": "2023-05-02T15:46:45.561117Z"
    }
   },
   "outputs": [],
   "source": [
    "normalization = False # Set to True to normalize the spectrogram\n",
    "\n",
    "#normalization limits\n",
    "minval = 0\n",
    "maxval = 1\n",
    "\n",
    "#Normalization\n",
    "\n",
    "def normalize_spectrogram(x, min_val = minval, max_val = maxval):\n",
    "    min_x = tf.reduce_min(x, axis=[1, 2], keepdims=True)\n",
    "    max_x = tf.reduce_max(x, axis=[1, 2], keepdims=True)\n",
    "    normalized_x = (x - min_x) * (max_val - min_val) / (max_x - min_x) + min_val\n",
    "    return normalized_x\n",
    "\n",
    "if(normalization):\n",
    "    train_spectrogram_ds = train_spectrogram_ds.map(lambda audio, label: (normalize_spectrogram(audio), label), tf.data.AUTOTUNE)\n",
    "    val_spectrogram_ds = val_spectrogram_ds.map(lambda audio, label: (normalize_spectrogram(audio), label), tf.data.AUTOTUNE)\n",
    "    test_spectrogram_ds = test_spectrogram_ds.map(lambda audio, label: (normalize_spectrogram(audio), label), tf.data.AUTOTUNE)\n",
    "\n",
    "input_shape = example_spectrograms.shape[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define 3 custom objects needed for the quantized model.\n",
    "- **level_quant_wrapper**: custom weights quantization function\n",
    "- **CustomLevelClip**: custom constraint function for the weights\n",
    "- **CustomQuantizeLayer**: this layer is a wrapper for quantizing the activations of each layer. We implemented it as a separate layer rather than as a function in the Larq layers to be able to observe its effect on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom quantization function\n",
    "\n",
    "def level_quant_wrapper(levels, is_activation=True):\n",
    "    @tf.custom_gradient\n",
    "    def level_quant(x):\n",
    "        \n",
    "        def grad(dy):\n",
    "            if is_activation:\n",
    "                return tf.where(tf.math.logical_and(x >= min(levels), x <= max(levels)), dy, 0)\n",
    "            else:\n",
    "                return dy\n",
    "        \n",
    "        def quant(x):\n",
    "            levels_tensor = tf.constant(levels, dtype=tf.float32)\n",
    "            distances = tf.abs(tf.expand_dims(x, -1) - levels_tensor)\n",
    "            min_distance_indices = tf.argmin(distances, axis=-1)\n",
    "            return tf.gather(levels_tensor, min_distance_indices)\n",
    "\n",
    "        return quant(x), grad\n",
    "\n",
    "    return level_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:46:46.692785Z",
     "start_time": "2023-05-02T15:46:46.686870Z"
    }
   },
   "outputs": [],
   "source": [
    "#Custom Clip function\n",
    "\n",
    "from keras.constraints import Constraint\n",
    "import keras.backend as k\n",
    "\n",
    "class CustomLevelClip(Constraint):\n",
    "    \n",
    "    def __init__(self, levels):\n",
    "        self.levels = levels\n",
    "\n",
    "    def __call__(self, w):        \n",
    "        return k.clip(w, min(self.levels), max(self.levels))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'levels': self.levels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom quantization layer\n",
    "\n",
    "class CustomQuantizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, quantizer, input_shape=None, **kwargs):\n",
    "        if input_shape is not None:\n",
    "            super(CustomQuantizeLayer, self).__init__(input_shape=input_shape, **kwargs)\n",
    "        else:\n",
    "            super(CustomQuantizeLayer, self).__init__(**kwargs)\n",
    "        self.quantizer = quantizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CustomQuantizeLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.quantizer(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"quantizer\": self.quantizer,\n",
    "            \"input_shape\": self.input_shape,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the model are quantized using a custom quantization function. In the LEVELS list you can specify the values that the weights can assume. You can specify as many values as you want.\n",
    "Do note that the more the values are concentrated around 0, the better the training accuracy is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom weight quantization parameters\n",
    "\n",
    "LEVELS = [-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "\n",
    "LEVELS.sort()\n",
    "print(\"Custom quantizer levels: \", LEVELS)\n",
    "\n",
    "max_value = max(LEVELS)\n",
    "min_value = min(LEVELS)\n",
    "\n",
    "custom_constraint = CustomLevelClip(levels=LEVELS)\n",
    "\n",
    "kwargs_no_input_quant = dict(\n",
    "    kernel_quantizer=level_quant_wrapper(levels=LEVELS, is_activation=False),\n",
    "    kernel_constraint=custom_constraint,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(minval=min_value, maxval=max_value, seed=np.random.randint(0, 1000)),\n",
    "    use_bias=False\n",
    ")\n",
    "\n",
    "#quantization of activations is done in a separate layer \n",
    "quantizer = lq.quantizers.DoReFa(k_bit=3, mode=\"activations\")\n",
    "\n",
    "#batch normalization arguments\n",
    "kwargs_BN = dict(\n",
    "    scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the model architecture.\n",
    "\n",
    "This architecture is well tested for 5 keywords (95% accuracy), but you can modify the parameters to fit your use case.\n",
    "\n",
    "**DROPOUT_RATE**: dropout rate for Conv2D layers\n",
    "\n",
    "**DROPOUT_RATE_DENSE**: dropout rate for Dense layers\n",
    "\n",
    "Note that the larq model summary function does not show the correct precision of the weights because standard quantizers work with a fixed number of bits instead of an arbitrary number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_RATE = 0.15\n",
    "DROPOUT_RATE_DENSE = 0.3\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(CustomQuantizeLayer(quantizer, input_shape=input_shape))\n",
    "model.add(lq.layers.QuantConv2D(32, (11, 11), **kwargs_no_input_quant))\n",
    "model.add(tf.keras.layers.MaxPooling2D(3, 3))\n",
    "model.add(tf.keras.layers.BatchNormalization(**kwargs_BN))\n",
    "#dropout\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "model.add(CustomQuantizeLayer(quantizer))\n",
    "model.add(lq.layers.QuantConv2D(32, (5, 5), **kwargs_no_input_quant))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(**kwargs_BN))\n",
    "#dropout\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "model.add(CustomQuantizeLayer(quantizer))\n",
    "model.add(lq.layers.QuantConv2D(32, (3, 3), **kwargs_no_input_quant))\n",
    "model.add(tf.keras.layers.BatchNormalization(**kwargs_BN))\n",
    "#dropout\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "\n",
    "#flatten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(CustomQuantizeLayer(quantizer))\n",
    "model.add(lq.layers.QuantDense(len(label_names), **kwargs_no_input_quant))\n",
    "model.add(tf.keras.layers.BatchNormalization(**kwargs_BN))\n",
    "#dropout\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT_RATE_DENSE))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#print number of trainable and non-trainable parameters\n",
    "print(\"Trainable parameters:\", model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compile and fit our model. We have tested with the Adam optimizer and a 0.005 learning rate. \n",
    "\n",
    "There is also some code to save tensorboard logs and to implement early stopping. \n",
    "\n",
    "Around 20 epochs should be enough to get good results with 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                        mode=\"min\", patience=5,\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "model.fit(train_spectrogram_ds,\n",
    "          epochs=20,\n",
    "          callbacks=[tensorboard_callback, earlystopping],\n",
    "          validation_data=val_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test dataset\n",
    "model.evaluate(test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_WEIGHTS_PATH = \"./saved_weights/\"\n",
    "SAVED_MODELS_PATH = \"./saved_models/\"\n",
    "FILE_NAME = \"model95.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save weights\n",
    "\n",
    "if not os.path.exists(SAVED_WEIGHTS_PATH):\n",
    "    os.makedirs(SAVED_WEIGHTS_PATH)\n",
    "\n",
    "model.save_weights(SAVED_WEIGHTS_PATH + FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "\n",
    "model.save(SAVED_MODELS_PATH + FILE_NAME.replace(\".h5\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved weights\n",
    "\n",
    "model.load_weights(SAVED_WEIGHTS_PATH + FILE_NAME)\n",
    "\n",
    "if not model._is_compiled:\n",
    "    model.compile(tf.keras.optimizers.legacy.Adam(learning_rate=0.005),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "\n",
    "with lq.context.quantized_scope(True):\n",
    "    model = tf.keras.models.load_model(\"model\")\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are stored as 32bit floating point numbers, to get the quantized weights we need to use larq's quantized_scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lq.context.quantized_scope(True):\n",
    "    quantized_weights = model.get_weights()\n",
    "print(quantized_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end we can calculate the confusion matrix of our model and all the intermediate outputs of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix function\n",
    "def get_true_and_predicted_labels(model, dataset):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for image_batch, label_batch in dataset:\n",
    "        y_true.append(tf.argmax(label_batch, axis=-1))  # Convert one-hot encoded labels to class indices\n",
    "        preds = model.predict(image_batch, verbose=0)\n",
    "        y_pred.append(np.argmax(preds, axis=-1))\n",
    "\n",
    "    correct_labels = tf.concat([item for item in y_true], axis=0)\n",
    "    predicted_labels = tf.concat([item for item in y_pred], axis=0)\n",
    "\n",
    "    return correct_labels, predicted_labels\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def print_confusion_matrix(model, val_spectrogram_ds):\n",
    "    correct_labels, predicted_labels = get_true_and_predicted_labels(model, val_spectrogram_ds)\n",
    "    confusion_mtx = tf.math.confusion_matrix(correct_labels, predicted_labels).numpy()\n",
    "\n",
    "     # Optionally, you can use ConfusionMatrixDisplay from scikit-learn to visualize the confusion matrix\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    display = ConfusionMatrixDisplay(confusion_mtx, display_labels=label_names)\n",
    "\n",
    "    display.plot(xticks_rotation='vertical', ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_confusion_matrix(model, test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the output of a layer\n",
    "\n",
    "def get_layer_output(model, layer_index, test_ds: tf.data.Dataset):\n",
    "\n",
    "    # Create a new model with the specified layer's output\n",
    "    layer_output_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[layer_index].output)\n",
    "\n",
    "    # Pass the input_data to the new model to get the output of the specified layer\n",
    "    example, label = test_ds.rebatch(1).shuffle(len(test_ds)).take(1).as_numpy_iterator().next()    \n",
    "\n",
    "    #convert label to string\n",
    "    label = label_names[np.argmax(label)]\n",
    "\n",
    "    #get layer output\n",
    "    layer_output = layer_output_model.predict(example)\n",
    "\n",
    "    return layer_output, label\n",
    "\n",
    "#functions to plot feature maps\n",
    "\n",
    "show_colorbar = True\n",
    "\n",
    "def plot_feature_maps(model, layer_index, test_spectrogram_ds: tf.data.Dataset):\n",
    "\n",
    "    #handle plotting of input\n",
    "    if layer_index == -1:\n",
    "        example, label = test_spectrogram_ds.rebatch(1).shuffle(len(test_spectrogram_ds)).take(1).as_numpy_iterator().next()\n",
    "        label = label_names[np.argmax(label)]\n",
    "        plot_conv_feature_maps(example, 1, layer_index, model, label)\n",
    "        return\n",
    "\n",
    "    feature_maps, label = get_layer_output(model, layer_index, test_spectrogram_ds)\n",
    "    \n",
    "    # Check the dimensions of the layer output\n",
    "    #if(\"CustomQuantizeLayer\" in str(type(model.layers[layer_index]))):\n",
    "    #    print(\"Specified layers is a CustomQuantizeLayer. Cannot plot feature maps.\")\n",
    "    #    return\n",
    "    if len(feature_maps.shape) == 4:  # Conv2D or MaxPooling2D layers\n",
    "        num_feature_maps = feature_maps.shape[-1]\n",
    "        plot_conv_feature_maps(feature_maps, num_feature_maps, layer_index, model, label)\n",
    "        \n",
    "    elif len(feature_maps.shape) == 2:  # Dense layer\n",
    "        plot_dense_feature_maps(feature_maps, layer_index, model, label)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Layer {layer_index} has an unsupported output shape. Cannot plot feature maps.\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_conv_feature_maps(feature_maps, num_feature_maps, layer_index, model, label, show_colorbar=show_colorbar):\n",
    "    # Create a grid of subplots\n",
    "    if num_feature_maps == 1:\n",
    "        num_cols = 1\n",
    "        num_rows = 1\n",
    "        figsize = (10, 7)  # Rectangular dimensions for single-channel input spectrogram\n",
    "    else:\n",
    "        num_cols = 4\n",
    "        num_rows = num_feature_maps // num_cols + (num_feature_maps % num_cols > 0)\n",
    "        figsize = (15, 15)  # Square dimensions for multi-channel feature maps\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "\n",
    "    # Set title of the figure as the layer name and index\n",
    "    if (layer_index != -1) & (layer_index != 0): fig.suptitle(f'Feature Maps of Layer {layer_index}: {model.layers[layer_index].name}\\nOutput shape {feature_maps.shape}\\nLabel: {label}')\n",
    "    elif layer_index == -1 : fig.suptitle(f'Input Spectrogram\\nLabel: {label}')\n",
    "    elif layer_index == 0 : fig.suptitle(f'Quantized Input Spectrogram\\nLabel: {label}')\n",
    "    \n",
    "    # Plot each feature map\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            idx = i * num_cols + j\n",
    "            if idx < num_feature_maps:\n",
    "                if num_feature_maps == 1:\n",
    "                    ax = axes\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "                img = ax.imshow(feature_maps[0, :, :, idx], cmap='gray')\n",
    "                if (layer_index != -1) & (layer_index != 0): ax.set_title(f'Feature Map {idx}')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                # Add a colorbar legend to the right of each image if show_colorbar is True\n",
    "                if show_colorbar:\n",
    "                    cbar = fig.colorbar(img, ax=ax)\n",
    "                    cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_dense_feature_maps(feature_maps, layer_index, model, label):\n",
    "    # Reshape the feature maps to a 1D array\n",
    "    reshaped_feature_maps = np.reshape(feature_maps, (-1,))\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Set title of the figure as the layer name and index\n",
    "    fig.suptitle(f'Feature Maps of Layer {layer_index}: {model.layers[layer_index].name}\\nOutput shape {feature_maps.shape}\\nLabel: {label}')\n",
    "\n",
    "    # Plot the feature maps as a bar plot\n",
    "    ax.bar(range(len(reshaped_feature_maps)), reshaped_feature_maps)\n",
    "    ax.set_xlabel('Feature Map Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    #set x ticks to be label names if layer is last one \n",
    "    if(layer_index == len(model.layers) - 1):\n",
    "        ax.set_xticks(np.arange(len(reshaped_feature_maps)), label_names)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the output of a specific layer we select a random (but constant between executions) sample by setting a seed.\n",
    "\n",
    "To select a different layer, change the layer index in the last line of code (-1 shows the input sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(399)\n",
    "plot_feature_maps(model, -1, test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model quantized weights for all layers\n",
    "with lq.context.quantized_scope(True):\n",
    "    for layer in model.layers:\n",
    "            print(layer.name)\n",
    "            print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print all layers activations\n",
    "\n",
    "def print_layers_activations(model: tf.keras.Sequential, test_ds: tf.data.Dataset, mode, index=None):\n",
    "    \n",
    "    if mode == \"all\":\n",
    "        for i in range(model.layers.__len__()):\n",
    "            print(f\"Layer {i}: {model.layers[i].name}\")\n",
    "            feature_maps, label = get_layer_output(model, i, test_ds)\n",
    "            print(feature_maps)\n",
    "            print(feature_maps.shape)\n",
    "            print(\"\\n\")\n",
    "    elif mode == \"single\":\n",
    "        if index == -1:\n",
    "            example, label = test_ds.rebatch(1).shuffle(len(test_ds)).take(1).as_numpy_iterator().next() \n",
    "            print(\"Spectrogram with label \" + label_names[np.argmax(label)])\n",
    "            print(\"\\n\")\n",
    "            print(example)\n",
    "            print(example.shape)\n",
    "            return\n",
    "        print(f\"Layer {index}: {model.layers[index].name}\")\n",
    "        feature_maps, label = get_layer_output(model, index, test_ds)\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        print(feature_maps)\n",
    "        print(feature_maps.shape)\n",
    "        print(\"\\n\")\n",
    "    print(label_names[np.argmax(label)])\n",
    "\n",
    "#Print all layers activations\n",
    "print_layers_activations(model, test_spectrogram_ds, \"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d525f717c636b61dd10d03170c153b82d88c7b14443ea3adfa4837365d4574ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
